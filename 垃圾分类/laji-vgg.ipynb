{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paddle 2.0.1\n",
      "using CUDAPlace(0) device.\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "print(\"paddle \" + paddle.__version__)\n",
    "device = paddle.set_device(\"gpu\")  # 指定设备\n",
    "print(\"using {} device.\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37] processing [312/312]\n",
      "[18] processing [352/352]\n",
      "[27] processing [526/526]\n",
      "[24] processing [308/308]\n",
      "[34] processing [385/385]\n",
      "[22] processing [365/365]\n",
      "[33] processing [312/312]\n",
      "[35] processing [341/341]\n",
      "[20] processing [216/216]\n",
      "[19] processing [302/302]\n",
      "[25] processing [540/540]\n",
      "[4] processing [377/377]\n",
      "[11] processing [726/726]\n",
      "[36] processing [255/255]\n",
      "[15] processing [409/409]\n",
      "[28] processing [372/372]\n",
      "[39] processing [427/427]\n",
      "[29] processing [406/406]\n",
      "[3] processing [75/75]\n",
      "[14] processing [347/347]\n",
      "[31] processing [436/436]\n",
      "[32] processing [270/270]\n",
      "[30] processing [311/311]\n",
      "[1] processing [360/360]\n",
      "[9] processing [379/379]\n",
      "[16] processing [342/342]\n",
      "[38] processing [381/381]\n",
      "[2] processing [269/269]\n",
      "[10] processing [377/377]\n",
      "[13] processing [399/399]\n",
      "[7] processing [352/352]\n",
      "[23] processing [299/299]\n",
      "[21] processing [647/647]\n",
      "[12] processing [321/321]\n",
      "[6] processing [385/385]\n",
      "[8] processing [370/370]\n",
      "[26] processing [341/341]\n",
      "[5] processing [279/279]\n",
      "[0] processing [232/232]\n",
      "[17] processing [299/299]\n",
      "processing done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"data/data35095/train.zip\") as zf:\n",
    "   zf.extractall()\n",
    "\n",
    "import os\n",
    "from shutil import copy, rmtree\n",
    "import random\n",
    "\n",
    "\n",
    "def mk_file(file_path: str):\n",
    "    if os.path.exists(file_path):\n",
    "        # 如果文件夹存在，则先删除原文件夹在重新创建\n",
    "        rmtree(file_path)\n",
    "    os.makedirs(file_path)\n",
    "\n",
    "! mv train data/dataset\n",
    "\n",
    "# 保证随机可复现\n",
    "random.seed(0)\n",
    "\n",
    "# 将数据集中10%的数据划分到验证集中\n",
    "split_rate = 0.1\n",
    "\n",
    "# 指向你解压后的flower_photos文件夹\n",
    "# cwd = os.getcwd()\n",
    "data_root = \"data/\"\n",
    "origin_garbage_path = \"data/dataset\"\n",
    "assert os.path.exists(origin_garbage_path)\n",
    "garbage_class = [cla for cla in os.listdir(origin_garbage_path)\n",
    "                if os.path.isdir(os.path.join(origin_garbage_path, cla))]\n",
    "\n",
    "# 建立保存训练集的文件夹\n",
    "train_root = os.path.join(data_root, \"train\")\n",
    "mk_file(train_root)\n",
    "for cla in garbage_class:\n",
    "    # 建立每个类别对应的文件夹\n",
    "    mk_file(os.path.join(train_root, cla))\n",
    "\n",
    "# 建立保存验证集的文件夹\n",
    "val_root = os.path.join(data_root, \"val\")\n",
    "mk_file(val_root)\n",
    "for cla in garbage_class:\n",
    "    # 建立每个类别对应的文件夹\n",
    "    mk_file(os.path.join(val_root, cla))\n",
    "\n",
    "for cla in garbage_class:\n",
    "    cla_path = os.path.join(origin_garbage_path, cla)\n",
    "    images = os.listdir(cla_path)\n",
    "    num = len(images)\n",
    "    # 随机采样验证集的索引\n",
    "    eval_index = random.sample(images, k=int(num*split_rate))\n",
    "    for index, image in enumerate(images):\n",
    "        if image in eval_index:\n",
    "            # 将分配至验证集中的文件复制到相应目录\n",
    "            image_path = os.path.join(cla_path, image)\n",
    "            new_path = os.path.join(val_root, cla)\n",
    "            copy(image_path, new_path)\n",
    "        else:\n",
    "            # 将分配至训练集中的文件复制到相应目录\n",
    "            image_path = os.path.join(cla_path, image)\n",
    "            new_path = os.path.join(train_root, cla)\n",
    "            copy(image_path, new_path)\n",
    "        print(\"\\r[{}] processing [{}/{}]\".format(cla, index+1, num), end=\"\")  # processing bar\n",
    "    print()\n",
    "\n",
    "print(\"processing done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import paddle.nn as nn  # 包含了神经网络的主要元素\n",
    "import paddle\n",
    "\n",
    "class VGG(nn.Layer):\n",
    "    def __init__(self, features, num_classes=1000):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),  # 减少过拟合\n",
    "            nn.Linear(512*7*7, 2048, weight_attr=nn.initializer.KaimingNormal()),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(2048, 2048, weight_attr=nn.initializer.KaimingNormal()),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, num_classes, weight_attr=nn.initializer.KaimingNormal())\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # N x 3 x 224 x 224\n",
    "        x = self.features(x)\n",
    "        # N x 512 x 7 x 7\n",
    "        x = paddle.flatten(x, start_axis=1)  # 展平处理，从第一个维度开始\n",
    "        # N x 512*7*7\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(cfg: list):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == \"M\":\n",
    "            layers += [nn.MaxPool2D(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2D = nn.Conv2D(in_channels, v, kernel_size=3, padding=1, weight_attr=nn.initializer.KaimingNormal())\n",
    "            layers += [conv2D, nn.ReLU()]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "# 产生提取特征网络结构\n",
    "# 遍历得到列表，结果通过sequential用非关键字参数传入\n",
    "# 以vgg-16为例：\n",
    "# 输入是224*224*3，经过一个3*3的卷积核，大小变为（224+2-3）+1=224，channel变为64；再通过一个3*3*64的卷积核仍是这样\n",
    "# max-pooling层：（224-2）/2+1=112\n",
    "# 两个卷积层之后channel变为128，feature map变为112*112*128\n",
    "# maxpooling层之后（112-2）/2+1=56\n",
    "# 三个卷积层后变为56*56*256\n",
    "# 以此类推：56*56*256 -> 28*28*256 -> 28*28*512 -> 14*14*512 -> 14*14*512 ->7*7*512\n",
    "# 最后进入全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = {\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "# 四种不同结构，数字代表卷积层中数字个数，M代表池化层\n",
    "\n",
    "def vgg(model_name=\"vgg16\", **kwargs):  # 实例化\n",
    "    try:\n",
    "        cfg = cfgs[model_name]\n",
    "    except:\n",
    "        print(\"Warning: model number {} not in cfgs dict!\".format(model_name))\n",
    "        exit(-1)\n",
    "    model = VGG(make_features(cfg), **kwargs)  # 传入features\n",
    "    # 后面的是字典变量\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, Mapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n",
      "2021-03-30 13:11:15,278 - INFO - font search path ['/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf', '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/afm', '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/pdfcorefonts']\n",
      "2021-03-30 13:11:15,728 - INFO - generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "\n",
    "from paddle.vision import transforms\n",
    "from paddle.vision import datasets\n",
    "import paddle.optimizer as optim\n",
    "from paddle.static import InputSpec\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "data_transform = {\n",
    "    \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "    \"val\": transforms.Compose([transforms.Resize((224, 224)),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}\n",
    "# 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 12980 images for training, 1422 images for validation.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.DatasetFolder(root=\"data/train\",\n",
    "                                     transform=data_transform[\"train\"])\n",
    "train_num = len(train_dataset)\n",
    "\n",
    "garbage_list = train_dataset.class_to_idx\n",
    "cla_dict = dict((val, key) for key, val in garbage_list.items())  # key和val反一下\n",
    "\n",
    "# write dict into json file\n",
    "\n",
    "json_str = json.dumps(cla_dict, indent=4)\n",
    "with open('class_indices.json', 'w') as json_file:\n",
    "    json_file.write(json_str)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = paddle.io.DataLoader(train_dataset,\n",
    "                                    batch_size=batch_size, shuffle=True,\n",
    "                                    num_workers=0)\n",
    "\n",
    "# windows只能为0，线程数(num_workers)\n",
    "\n",
    "validate_dataset = datasets.DatasetFolder(root=\"data/val\",\n",
    "                                          transform=data_transform[\"val\"])\n",
    "val_num = len(validate_dataset)\n",
    "validate_loader = paddle.io.DataLoader(validate_dataset,\n",
    "                                        batch_size=batch_size, shuffle=True,\n",
    "                                        num_workers=0)\n",
    "print(\"using {} images for training, {} images for validation.\".format(train_num,\n",
    "                                                                       val_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  1 %[->.................................................]3.961 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8c71bd0adf73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# train_bar = tqdm(train_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0min_dygraph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_next_var_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_name = \"vgg16\"\n",
    "net = vgg(model_name=model_name, num_classes=40)  # 进行实例化\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(parameters=net.parameters(), learning_rate=0.0001)\n",
    "\n",
    "epochs = 200\n",
    "best_acc = 0.0\n",
    "# save_path = './{}Net.pth'.format(model_name)\n",
    "train_steps = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    # train\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    t1 = time.perf_counter()\n",
    "    # train_bar = tqdm(train_loader)\n",
    "    for step, data in enumerate(train_loader, start=0):\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad() # 梯度置零\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.numpy()[0]\n",
    "        rate = (step + 1) / len(train_loader)\n",
    "        a = \"*\" * int(rate * 50)\n",
    "        b = \".\" * int((1 - rate) * 50)\n",
    "        loss_num = loss.numpy()[0]\n",
    "        print(\"\\rtrain loss: {:^3.0f}%[{}->{}]{:.3f}\".format(int(rate * 100), a, b, loss_num), end=\" \")\n",
    "    print()\n",
    "    print(time.perf_counter()-t1)\n",
    "\n",
    "    # validate\n",
    "    net.eval()\n",
    "    acc = 0.0  # accumulate accurate number / epoch\n",
    "    with paddle.no_grad():\n",
    "        for val_data in validate_loader:\n",
    "            val_images, val_labels = val_data\n",
    "            outputs = net(val_images)\n",
    "            predict_y = outputs.numpy().argmax(axis=1)\n",
    "            acc += (val_labels.astype(\"int\").numpy()==outputs.numpy().argmax(axis=1).astype(\"int\")).sum()\n",
    "\n",
    "    val_accurate = acc / val_num\n",
    "    print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %\n",
    "          (epoch + 1, running_loss / train_steps, val_accurate))\n",
    "\n",
    "\n",
    "    if val_accurate > best_acc:\n",
    "        best_acc = val_accurate\n",
    "        paddle.jit.save(\n",
    "        layer=net,\n",
    "        path=\"model\",\n",
    "        input_spec=[InputSpec(shape=[1, 3, 224, 224], dtype='float32')])\n",
    "        \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv data/data77426/model.pdiparams model.pdiparams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/backward.py:1640: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return list(x) if isinstance(x, collections.Sequence) else [x]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process: 23% [***********->......................................]  "
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"data/data35095/test.zip\") as zf:\n",
    "   zf.extractall()\n",
    "\n",
    "data_transform = transforms.Compose(\n",
    "    [transforms.Resize((224, 224)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# load image\n",
    "with open(\"test.txt\",\"w\") as f:\n",
    "    for i in range(1,401):\n",
    "        img_path = \"test/test\"+str(i)+\".jpg\"\n",
    "        assert os.path.exists(img_path), \"file: '{}' dose not exist.\".format(img_path)\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        paddle.disable_static()\n",
    "        # plt.imshow(img)\n",
    "        # [N, C, H, W]\n",
    "        img = data_transform(img)\n",
    "        # expand batch dimension\n",
    "        img = paddle.unsqueeze(img, axis=0)\n",
    "\n",
    "        # read class_indict\n",
    "        json_path = 'class_indices.json'\n",
    "        assert os.path.exists(json_path), \"file: '{}' dose not exist.\".format(json_path)\n",
    "\n",
    "        json_file = open(json_path, \"r\")\n",
    "        class_indict = json.load(json_file)\n",
    "\n",
    "        # create model\n",
    "        model = vgg(model_name='vgg16', num_classes=40)\n",
    "\n",
    "        # load model weights\n",
    "        # weights_path = \"AlexNet.pth\"\n",
    "        # assert os.path.exists(weights_path), \"file: '{}' dose not exist.\".format(weights_path)\n",
    "        model = paddle.jit.load(\"model\")\n",
    "        # model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "        model.eval()\n",
    "        with paddle.no_grad():\n",
    "            # predict class\n",
    "            output = paddle.squeeze(model(img))\n",
    "            predict = nn.Softmax(axis=0)(output)\n",
    "            predict_cla = paddle.argmax(predict).numpy()[0]\n",
    "\n",
    "        # print_res = \"class: {}   prob: {:.3}\".format(class_indict[str(predict_cla)],\n",
    "                                                    #predict.numpy()[predict_cla])\n",
    "        # print(print_res)\n",
    "    \n",
    "        f.write(class_indict[str(predict_cla)]+\"\\n\")\n",
    "\n",
    "        rateTest = i / 400\n",
    "        aTest = \"*\" * int(rateTest * 50)\n",
    "        bTest = \".\" * int((1 - rateTest) * 50)\n",
    "        print(\"\\rprocess: {:d}% [{}->{}]\".format(int(rateTest * 100), aTest, bTest), end=\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

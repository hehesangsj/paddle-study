{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data35095  dataset  train  val\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Requirement already satisfied: paddlepaddle-gpu in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.0.1.post101)\n",
      "Requirement already satisfied: numpy>=1.13; python_version >= \"3.5\" and platform_system != \"Windows\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu) (1.16.4)\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu) (4.4.0)\n",
      "Requirement already satisfied: gast>=0.3.3; platform_system != \"Windows\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu) (0.3.3)\n",
      "Requirement already satisfied: Pillow in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu) (7.1.2)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu) (3.14.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu) (2.22.0)\n",
      "Requirement already satisfied: astor in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu) (0.8.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle-gpu) (1.25.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle-gpu) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle-gpu) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle-gpu) (2019.9.11)\n"
     ]
    }
   ],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "# !mkdir /home/aistudio/external-libraries\n",
    "# !pip install beautifulsoup4 -t /home/aistudio/external-libraries\n",
    "!pip install paddlepaddle-gpu\n",
    "import paddle as paddle\n",
    "# Initialize PaddlePaddle.\n",
    "with_gpu = os.getenv('WITH_GPU', '1') != '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paddle 2.0.1\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "print(\"paddle \" + paddle.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CUDAPlace(0) device.\n"
     ]
    }
   ],
   "source": [
    "device = paddle.set_device(\"gpu\")  # 指定设备\r\n",
    "print(\"using {} device.\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n",
      "16.0\n",
      "20.0\n",
      "28.0\n",
      "30.0\n",
      "50.0\n",
      "paddle 2.0.1\n",
      "w before optimize: 0.9323206543922424\n",
      "b before optimize: 0.0\n",
      "epoch 0 loss [387.86243]\n",
      "epoch 1000 loss [8.180787]\n",
      "epoch 2000 loss [1.8291637]\n",
      "epoch 3000 loss [0.40899396]\n",
      "epoch 4000 loss [0.09144972]\n",
      "finished training， loss [0.02047826]\n",
      "w after optimize: 2.0182228088378906\n",
      "b after optimize: 9.766996383666992\n",
      "hello paddle\n"
     ]
    }
   ],
   "source": [
    "def calculate_fee(distance_travelled):\r\n",
    "    return 10 + 2 * distance_travelled\r\n",
    "\r\n",
    "for x in [1.0, 3.0, 5.0, 9.0, 10.0, 20.0]:\r\n",
    "    print(calculate_fee(x))\r\n",
    "    \r\n",
    "import paddle\r\n",
    "print(\"paddle \" + paddle.__version__)\r\n",
    "x_data = paddle.to_tensor([[1.], [3.0], [5.0], [9.0], [10.0], [20.0]])\r\n",
    "y_data = paddle.to_tensor([[12.], [16.0], [20.0], [28.0], [30.0], [50.0]])\r\n",
    "linear = paddle.nn.Linear(in_features=1, out_features=1)\r\n",
    "w_before_opt = linear.weight.numpy().item()\r\n",
    "b_before_opt = linear.bias.numpy().item()\r\n",
    "\r\n",
    "print(\"w before optimize: {}\".format(w_before_opt))\r\n",
    "print(\"b before optimize: {}\".format(b_before_opt))\r\n",
    "mse_loss = paddle.nn.MSELoss()\r\n",
    "sgd_optimizer = paddle.optimizer.SGD(learning_rate=0.001, parameters = linear.parameters())\r\n",
    "total_epoch = 5000\r\n",
    "for i in range(total_epoch):\r\n",
    "    y_predict = linear(x_data)\r\n",
    "    loss = mse_loss(y_predict, y_data)\r\n",
    "    loss.backward()\r\n",
    "    sgd_optimizer.step()\r\n",
    "    sgd_optimizer.clear_grad()\r\n",
    "\r\n",
    "    if i%1000 == 0:\r\n",
    "        print(\"epoch {} loss {}\".format(i, loss.numpy()))\r\n",
    "\r\n",
    "print(\"finished training， loss {}\".format(loss.numpy()))\r\n",
    "w_after_opt = linear.weight.numpy().item()\r\n",
    "b_after_opt = linear.bias.numpy().item()\r\n",
    "\r\n",
    "print(\"w after optimize: {}\".format(w_after_opt))\r\n",
    "print(\"b after optimize: {}\".format(b_after_opt))\r\n",
    "print(\"hello paddle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\r\n",
    "with zipfile.ZipFile(\"data/data35095/train.zip\") as zf:\r\n",
    "   zf.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! mv train data/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36] processing [255/255]\n",
      "[14] processing [347/347]\n",
      "[34] processing [385/385]\n",
      "[11] processing [726/726]\n",
      "[30] processing [311/311]\n",
      "[35] processing [341/341]\n",
      "[10] processing [377/377]\n",
      "[26] processing [341/341]\n",
      "[2] processing [269/269]\n",
      "[24] processing [308/308]\n",
      "[21] processing [647/647]\n",
      "[33] processing [312/312]\n",
      "[4] processing [377/377]\n",
      "[25] processing [540/540]\n",
      "[37] processing [312/312]\n",
      "[5] processing [279/279]\n",
      "[0] processing [232/232]\n",
      "[16] processing [342/342]\n",
      "[28] processing [372/372]\n",
      "[20] processing [216/216]\n",
      "[13] processing [399/399]\n",
      "[19] processing [302/302]\n",
      "[18] processing [352/352]\n",
      "[32] processing [270/270]\n",
      "[29] processing [406/406]\n",
      "[38] processing [381/381]\n",
      "[23] processing [299/299]\n",
      "[6] processing [385/385]\n",
      "[3] processing [75/75]\n",
      "[22] processing [365/365]\n",
      "[27] processing [526/526]\n",
      "[12] processing [321/321]\n",
      "[39] processing [427/427]\n",
      "[15] processing [409/409]\n",
      "[31] processing [436/436]\n",
      "[8] processing [370/370]\n",
      "[9] processing [379/379]\n",
      "[17] processing [299/299]\n",
      "[7] processing [352/352]\n",
      "[1] processing [360/360]\n",
      "processing done!\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "from shutil import copy, rmtree\r\n",
    "import random\r\n",
    "\r\n",
    "\r\n",
    "def mk_file(file_path: str):\r\n",
    "    if os.path.exists(file_path):\r\n",
    "        # 如果文件夹存在，则先删除原文件夹在重新创建\r\n",
    "        rmtree(file_path)\r\n",
    "    os.makedirs(file_path)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# 保证随机可复现\r\n",
    "random.seed(0)\r\n",
    "\r\n",
    "# 将数据集中10%的数据划分到验证集中\r\n",
    "split_rate = 0.1\r\n",
    "\r\n",
    "# 指向你解压后的flower_photos文件夹\r\n",
    "# cwd = os.getcwd()\r\n",
    "data_root = \"data/\"\r\n",
    "origin_garbage_path = \"data/dataset\"\r\n",
    "assert os.path.exists(origin_garbage_path)\r\n",
    "garbage_class = [cla for cla in os.listdir(origin_garbage_path)\r\n",
    "                if os.path.isdir(os.path.join(origin_garbage_path, cla))]\r\n",
    "\r\n",
    "# 建立保存训练集的文件夹\r\n",
    "train_root = os.path.join(data_root, \"train\")\r\n",
    "mk_file(train_root)\r\n",
    "for cla in garbage_class:\r\n",
    "    # 建立每个类别对应的文件夹\r\n",
    "    mk_file(os.path.join(train_root, cla))\r\n",
    "\r\n",
    "# 建立保存验证集的文件夹\r\n",
    "val_root = os.path.join(data_root, \"val\")\r\n",
    "mk_file(val_root)\r\n",
    "for cla in garbage_class:\r\n",
    "    # 建立每个类别对应的文件夹\r\n",
    "    mk_file(os.path.join(val_root, cla))\r\n",
    "\r\n",
    "for cla in garbage_class:\r\n",
    "    cla_path = os.path.join(origin_garbage_path, cla)\r\n",
    "    images = os.listdir(cla_path)\r\n",
    "    num = len(images)\r\n",
    "    # 随机采样验证集的索引\r\n",
    "    eval_index = random.sample(images, k=int(num*split_rate))\r\n",
    "    for index, image in enumerate(images):\r\n",
    "        if image in eval_index:\r\n",
    "            # 将分配至验证集中的文件复制到相应目录\r\n",
    "            image_path = os.path.join(cla_path, image)\r\n",
    "            new_path = os.path.join(val_root, cla)\r\n",
    "            copy(image_path, new_path)\r\n",
    "        else:\r\n",
    "            # 将分配至训练集中的文件复制到相应目录\r\n",
    "            image_path = os.path.join(cla_path, image)\r\n",
    "            new_path = os.path.join(train_root, cla)\r\n",
    "            copy(image_path, new_path)\r\n",
    "        print(\"\\r[{}] processing [{}/{}]\".format(cla, index+1, num), end=\"\")  # processing bar\r\n",
    "    print()\r\n",
    "\r\n",
    "print(\"processing done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn as nn  # 包含了神经网络的主要元素\r\n",
    "import paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Layer):  # 集成module可以直接调用父类函数，module是所有神经网络的基类\r\n",
    "    def __init__(self, num_classes=1000):  # 初始化模型\r\n",
    "        super(AlexNet, self).__init__()  # 使用父类的方法对子类进行初始化\r\n",
    "        self.features = nn.Sequential(  # 一系列的结构打包\r\n",
    "            nn.Conv2D(3, 48, kernel_size=11, stride=4, padding=2, weight_attr=nn.initializer.KaimingNormal()),  # input[3, 224, 224]  output[48, 55, 55]\r\n",
    "            # (224-11+4)/4+1=55\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.MaxPool2D(kernel_size=3, stride=2),                  # output[48, 27, 27]\r\n",
    "            # (55-3)/2+1=27\r\n",
    "            nn.Conv2D(48, 128, kernel_size=5, padding=2, weight_attr=nn.initializer.KaimingNormal()),           # output[128, 27, 27]\r\n",
    "            # (27+4-5)+1=27\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.MaxPool2D(kernel_size=3, stride=2),                  # output[128, 13, 13]\r\n",
    "            # (27-3)/2+1=13\r\n",
    "            nn.Conv2D(128, 192, kernel_size=3, padding=1, weight_attr=nn.initializer.KaimingNormal()),          # output[192, 13, 13]\r\n",
    "            # (13+2-3)+1=13\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Conv2D(192, 192, kernel_size=3, padding=1, weight_attr=nn.initializer.KaimingNormal()),          # output[192, 13, 13]\r\n",
    "            # (13+2-3)+1=13\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Conv2D(192, 128, kernel_size=3, padding=1, weight_attr=nn.initializer.KaimingNormal()),          # output[128, 13, 13]\r\n",
    "            # (13+2-3)+1=13\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.MaxPool2D(kernel_size=3, stride=2),                  # output[128, 6, 6]\r\n",
    "            # (13-3)/2+1=6\r\n",
    "        )\r\n",
    "        self.classifier = nn.Sequential(  # 全连接层分类器\r\n",
    "            nn.Dropout(p=0.5),  # 随即失活的比例\r\n",
    "            nn.Linear(128 * 6 * 6, 2048, weight_attr=nn.initializer.KaimingNormal()),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Dropout(p=0.5),\r\n",
    "            nn.Linear(2048, 2048, weight_attr=nn.initializer.KaimingNormal()),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(2048, num_classes, weight_attr=nn.initializer.KaimingNormal()),\r\n",
    "        )\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.features(x)\r\n",
    "        x = paddle.flatten(x, start_axis=1)  # 展平\r\n",
    "        x = self.classifier(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, Mapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n",
      "2021-03-29 09:46:27,634 - INFO - font search path ['/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf', '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/afm', '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/pdfcorefonts']\n",
      "2021-03-29 09:46:28,195 - INFO - generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "import json\r\n",
    "import time\r\n",
    "\r\n",
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "\r\n",
    "from paddle.vision import transforms\r\n",
    "from paddle.vision import datasets\r\n",
    "import paddle.optimizer as optim\r\n",
    "from paddle.static import InputSpec\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_transform = {\r\n",
    "    # 训练集预处理\r\n",
    "    \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\r\n",
    "                                 transforms.RandomHorizontalFlip(),\r\n",
    "                                 transforms.ToTensor(),\r\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\r\n",
    "    # 验证集预处理\r\n",
    "    \"val\": transforms.Compose([transforms.Resize((224, 224)),  # cannot 224, must (224, 224)\r\n",
    "                               transforms.ToTensor(),\r\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " data_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))  # get data root path\r\n",
    "# image_path = os.path.join(data_root)  # flower data set path\r\n",
    "# assert os.path.exists(image_path), \"{} path does not exist.\".format(image_path)\r\n",
    "train_dataset = datasets.DatasetFolder(root=\"data/train\",\r\n",
    "                                     transform=data_transform[\"train\"])\r\n",
    "train_num = len(train_dataset)\r\n",
    "garbage_list = train_dataset.class_to_idx\r\n",
    "cla_dict = dict((val, key) for key, val in garbage_list.items())  # key和val反一下\r\n",
    "# write dict into json file\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_str = json.dumps(cla_dict, indent=4)\r\n",
    "with open('class_indices.json', 'w') as json_file:\r\n",
    "    json_file.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 12980 images for training, 1422 images for validation.\n"
     ]
    }
   ],
   "source": [
    "\r\n",
    "batch_size = 128\r\n",
    "train_loader = paddle.io.DataLoader(train_dataset,\r\n",
    "                                    batch_size=batch_size, shuffle=True,\r\n",
    "                                    num_workers=0)\r\n",
    "validate_dataset = datasets.DatasetFolder(root=\"data/val\",\r\n",
    "                                          transform=data_transform[\"val\"])\r\n",
    "val_num = len(validate_dataset)\r\n",
    "validate_loader = paddle.io.DataLoader(validate_dataset,\r\n",
    "                                        batch_size=batch_size, shuffle=True,\r\n",
    "                                        num_workers=0)\r\n",
    "print(\"using {} images for training, {} images for validation.\".format(train_num,\r\n",
    "                                                                       val_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 100%[**************************************************->]3.357 \n",
      "89.20627645403147\n",
      "[epoch 1] train_loss: 3.627  val_accuracy: 0.122\n",
      "train loss: 100%[**************************************************->]3.296 \n",
      "88.93507681041956\n",
      "[epoch 2] train_loss: 3.350  val_accuracy: 0.130\n",
      "train loss: 100%[**************************************************->]3.209 \n",
      "88.60019026696682\n",
      "[epoch 3] train_loss: 3.231  val_accuracy: 0.186\n",
      "train loss: 100%[**************************************************->]3.084 \n",
      "89.09733299165964\n",
      "[epoch 4] train_loss: 3.151  val_accuracy: 0.190\n",
      "train loss: 100%[**************************************************->]2.972 \n",
      "88.08202955126762\n",
      "[epoch 5] train_loss: 3.064  val_accuracy: 0.202\n",
      "train loss: 100%[**************************************************->]2.863 \n",
      "88.71006862819195\n",
      "[epoch 6] train_loss: 2.997  val_accuracy: 0.221\n",
      "train loss: 100%[**************************************************->]2.903 \n",
      "89.40387020260096\n",
      "[epoch 7] train_loss: 2.940  val_accuracy: 0.251\n",
      "train loss: 100%[**************************************************->]2.839 \n",
      "88.80792818963528\n",
      "[epoch 8] train_loss: 2.904  val_accuracy: 0.231\n",
      "train loss: 100%[**************************************************->]2.775 \n",
      "88.46545926481485\n",
      "[epoch 9] train_loss: 2.838  val_accuracy: 0.262\n",
      "train loss: 100%[**************************************************->]2.755 \n",
      "89.21591301262379\n",
      "[epoch 10] train_loss: 2.813  val_accuracy: 0.267\n",
      "train loss: 100%[**************************************************->]2.653 \n",
      "88.8976288586855\n",
      "[epoch 11] train_loss: 2.764  val_accuracy: 0.282\n",
      "train loss: 100%[**************************************************->]2.281 \n",
      "89.92297897487879\n",
      "[epoch 12] train_loss: 2.714  val_accuracy: 0.276\n",
      "train loss: 100%[**************************************************->]2.645 \n",
      "90.8413987159729\n",
      "[epoch 13] train_loss: 2.690  val_accuracy: 0.299\n",
      "train loss: 100%[**************************************************->]2.710 \n",
      "89.56355686485767\n",
      "[epoch 14] train_loss: 2.646  val_accuracy: 0.293\n",
      "train loss: 100%[**************************************************->]2.942 \n",
      "89.54459435492754\n",
      "[epoch 15] train_loss: 2.608  val_accuracy: 0.325\n",
      "train loss: 100%[**************************************************->]2.581 \n",
      "89.44357070326805\n",
      "[epoch 16] train_loss: 2.580  val_accuracy: 0.313\n",
      "train loss: 100%[**************************************************->]2.763 \n",
      "88.71994628757238\n",
      "[epoch 17] train_loss: 2.550  val_accuracy: 0.329\n",
      "train loss: 100%[**************************************************->]2.566 \n",
      "88.84032597392797\n",
      "[epoch 18] train_loss: 2.520  val_accuracy: 0.345\n",
      "train loss: 100%[**************************************************->]2.559 \n",
      "90.10290810465813\n",
      "[epoch 19] train_loss: 2.508  val_accuracy: 0.357\n",
      "train loss: 100%[**************************************************->]2.564 \n",
      "89.7665875479579\n",
      "[epoch 20] train_loss: 2.468  val_accuracy: 0.356\n",
      "train loss: 100%[**************************************************->]2.383 \n",
      "89.60524180531502\n",
      "[epoch 21] train_loss: 2.437  val_accuracy: 0.371\n",
      "train loss: 100%[**************************************************->]2.527 \n",
      "89.25621499866247\n",
      "[epoch 22] train_loss: 2.422  val_accuracy: 0.375\n",
      "train loss: 100%[**************************************************->]2.713 \n",
      "90.41956658661366\n",
      "[epoch 23] train_loss: 2.389  val_accuracy: 0.369\n",
      "train loss: 100%[**************************************************->]2.127 \n",
      "92.51329506188631\n",
      "[epoch 24] train_loss: 2.359  val_accuracy: 0.366\n",
      "train loss: 100%[**************************************************->]2.173 \n",
      "90.27092853933573\n",
      "[epoch 25] train_loss: 2.340  val_accuracy: 0.370\n",
      "train loss: 100%[**************************************************->]2.324 \n",
      "90.35684308409691\n",
      "[epoch 26] train_loss: 2.310  val_accuracy: 0.381\n",
      "train loss: 100%[**************************************************->]2.428 \n",
      "90.5568785443902\n",
      "[epoch 27] train_loss: 2.298  val_accuracy: 0.398\n",
      "train loss: 100%[**************************************************->]2.107 \n",
      "89.70947027206421\n",
      "[epoch 28] train_loss: 2.290  val_accuracy: 0.387\n",
      "train loss: 100%[**************************************************->]2.241 \n",
      "90.54988549649715\n",
      "[epoch 29] train_loss: 2.250  val_accuracy: 0.388\n",
      "train loss: 100%[**************************************************->]2.191 \n",
      "90.75041768699884\n",
      "[epoch 30] train_loss: 2.232  val_accuracy: 0.390\n",
      "train loss: 100%[**************************************************->]1.748 \n",
      "90.86816071718931\n",
      "[epoch 31] train_loss: 2.222  val_accuracy: 0.409\n",
      "train loss: 100%[**************************************************->]2.231 \n",
      "90.77203754335642\n",
      "[epoch 32] train_loss: 2.209  val_accuracy: 0.411\n",
      "train loss: 100%[**************************************************->]2.141 \n",
      "90.76337683945894\n",
      "[epoch 33] train_loss: 2.176  val_accuracy: 0.411\n",
      "train loss: 100%[**************************************************->]1.986 \n",
      "91.37689171731472\n",
      "[epoch 34] train_loss: 2.164  val_accuracy: 0.423\n",
      "train loss: 100%[**************************************************->]1.946 \n",
      "92.1146039813757\n",
      "[epoch 35] train_loss: 2.139  val_accuracy: 0.410\n",
      "train loss: 100%[**************************************************->]2.029 \n",
      "90.72480861842632\n",
      "[epoch 36] train_loss: 2.127  val_accuracy: 0.435\n",
      "train loss: 100%[**************************************************->]2.264 \n",
      "90.33196390420198\n",
      "[epoch 37] train_loss: 2.098  val_accuracy: 0.432\n",
      "train loss: 100%[**************************************************->]1.829 \n",
      "91.37960074841976\n",
      "[epoch 38] train_loss: 2.095  val_accuracy: 0.419\n",
      "train loss: 100%[**************************************************->]2.058 \n",
      "90.51375330984592\n",
      "[epoch 39] train_loss: 2.087  val_accuracy: 0.422\n",
      "train loss: 100%[**************************************************->]2.333 \n",
      "91.01179479807615\n",
      "[epoch 40] train_loss: 2.063  val_accuracy: 0.437\n",
      "train loss: 100%[**************************************************->]1.972 \n",
      "91.61751455068588\n",
      "[epoch 41] train_loss: 2.047  val_accuracy: 0.444\n",
      "train loss: 100%[**************************************************->]1.762 \n",
      "90.94846919178963\n",
      "[epoch 42] train_loss: 2.027  val_accuracy: 0.440\n",
      "train loss: 100%[**************************************************->]2.126 \n",
      "90.68680191040039\n",
      "[epoch 43] train_loss: 2.018  val_accuracy: 0.442\n",
      "train loss: 100%[**************************************************->]2.110 \n",
      "90.59498662501574\n",
      "[epoch 44] train_loss: 1.990  val_accuracy: 0.451\n",
      "train loss: 100%[**************************************************->]2.172 \n",
      "91.37003298848867\n",
      "[epoch 45] train_loss: 1.991  val_accuracy: 0.450\n",
      "train loss: 100%[**************************************************->]1.957 \n",
      "90.64802246540785\n",
      "[epoch 46] train_loss: 1.973  val_accuracy: 0.444\n",
      "train loss: 100%[**************************************************->]2.258 \n",
      "90.86539908498526\n",
      "[epoch 47] train_loss: 1.976  val_accuracy: 0.444\n",
      "train loss: 100%[**************************************************->]1.949 \n",
      "90.64150190353394\n",
      "[epoch 48] train_loss: 1.962  val_accuracy: 0.454\n",
      "train loss: 100%[**************************************************->]1.846 \n",
      "91.25266796350479\n",
      "[epoch 49] train_loss: 1.945  val_accuracy: 0.463\n",
      "train loss: 100%[**************************************************->]1.888 \n",
      "90.70776755362749\n",
      "[epoch 50] train_loss: 1.907  val_accuracy: 0.455\n",
      "train loss: 100%[**************************************************->]1.815 \n",
      "90.46463502943516\n",
      "[epoch 51] train_loss: 1.905  val_accuracy: 0.459\n",
      "train loss: 100%[**************************************************->]1.890 \n",
      "91.16082467883825\n",
      "[epoch 52] train_loss: 1.906  val_accuracy: 0.477\n",
      "train loss: 100%[**************************************************->]1.649 \n",
      "89.55949161201715\n",
      "[epoch 53] train_loss: 1.889  val_accuracy: 0.458\n",
      "train loss: 100%[**************************************************->]2.403 \n",
      "91.00839883834124\n",
      "[epoch 54] train_loss: 1.880  val_accuracy: 0.462\n",
      "train loss: 100%[**************************************************->]1.774 \n",
      "90.14046780019999\n",
      "[epoch 55] train_loss: 1.863  val_accuracy: 0.463\n",
      "train loss: 100%[**************************************************->]1.626 \n",
      "91.44574436545372\n",
      "[epoch 56] train_loss: 1.837  val_accuracy: 0.470\n",
      "train loss: 100%[**************************************************->]1.777 \n",
      "91.62324492633343\n",
      "[epoch 57] train_loss: 1.846  val_accuracy: 0.479\n",
      "train loss: 100%[**************************************************->]1.833 \n",
      "89.28109399974346\n",
      "[epoch 58] train_loss: 1.825  val_accuracy: 0.466\n",
      "train loss: 100%[**************************************************->]1.737 \n",
      "90.35330368578434\n",
      "[epoch 59] train_loss: 1.806  val_accuracy: 0.469\n",
      "train loss: 100%[**************************************************->]1.849 \n",
      "92.26520320773125\n",
      "[epoch 60] train_loss: 1.809  val_accuracy: 0.482\n",
      "train loss: 100%[**************************************************->]1.473 \n",
      "89.28990509361029\n",
      "[epoch 61] train_loss: 1.795  val_accuracy: 0.482\n",
      "train loss: 100%[**************************************************->]1.497 \n",
      "90.74750272929668\n",
      "[epoch 62] train_loss: 1.786  val_accuracy: 0.488\n",
      "train loss: 100%[**************************************************->]1.831 \n",
      "90.87844326347113\n",
      "[epoch 63] train_loss: 1.779  val_accuracy: 0.479\n",
      "train loss: 100%[**************************************************->]1.651 \n",
      "89.83987172693014\n",
      "[epoch 64] train_loss: 1.784  val_accuracy: 0.487\n",
      "train loss: 100%[**************************************************->]1.830 \n",
      "89.29245899617672\n",
      "[epoch 65] train_loss: 1.766  val_accuracy: 0.490\n",
      "train loss: 100%[**************************************************->]1.875 \n",
      "89.92255666851997\n",
      "[epoch 66] train_loss: 1.730  val_accuracy: 0.485\n",
      "train loss: 100%[**************************************************->]2.132 \n",
      "90.34915030002594\n",
      "[epoch 67] train_loss: 1.743  val_accuracy: 0.480\n",
      "train loss: 100%[**************************************************->]1.753 \n",
      "89.26064702868462\n",
      "[epoch 68] train_loss: 1.732  val_accuracy: 0.491\n",
      "train loss: 100%[**************************************************->]1.477 \n",
      "93.66812870651484\n",
      "[epoch 69] train_loss: 1.721  val_accuracy: 0.482\n",
      "train loss: 100%[**************************************************->]1.975 \n",
      "89.72911374270916\n",
      "[epoch 70] train_loss: 1.697  val_accuracy: 0.485\n",
      "train loss: 100%[**************************************************->]1.596 \n",
      "90.56151406466961\n",
      "[epoch 71] train_loss: 1.673  val_accuracy: 0.500\n",
      "train loss: 100%[**************************************************->]1.972 \n",
      "90.8417874649167\n",
      "[epoch 72] train_loss: 1.681  val_accuracy: 0.489\n",
      "train loss: 100%[**************************************************->]1.706 \n",
      "89.8524863794446\n",
      "[epoch 73] train_loss: 1.681  val_accuracy: 0.488\n",
      "train loss: 100%[**************************************************->]1.607 \n",
      "89.58776723593473\n",
      "[epoch 74] train_loss: 1.681  val_accuracy: 0.497\n",
      "train loss: 100%[**************************************************->]1.971 \n",
      "89.29081447422504\n",
      "[epoch 75] train_loss: 1.658  val_accuracy: 0.500\n",
      "train loss: 100%[**************************************************->]1.950 \n",
      "89.34760537743568\n",
      "[epoch 76] train_loss: 1.663  val_accuracy: 0.496\n",
      "train loss: 100%[**************************************************->]2.001 \n",
      "89.94994644075632\n",
      "[epoch 77] train_loss: 1.635  val_accuracy: 0.485\n",
      "train loss: 100%[**************************************************->]1.468 \n",
      "91.00319353491068\n",
      "[epoch 78] train_loss: 1.633  val_accuracy: 0.500\n",
      "train loss: 100%[**************************************************->]1.608 \n",
      "90.9989952519536\n",
      "[epoch 79] train_loss: 1.651  val_accuracy: 0.499\n",
      "train loss: 100%[**************************************************->]1.853 \n",
      "90.75858342647552\n",
      "[epoch 80] train_loss: 1.605  val_accuracy: 0.511\n",
      "train loss: 100%[**************************************************->]1.649 \n",
      "90.10078059881926\n",
      "[epoch 81] train_loss: 1.607  val_accuracy: 0.530\n",
      "train loss: 100%[**************************************************->]1.403 \n",
      "90.0264832675457\n",
      "[epoch 82] train_loss: 1.591  val_accuracy: 0.527\n",
      "train loss: 100%[**************************************************->]1.604 \n",
      "89.95823159813881\n",
      "[epoch 83] train_loss: 1.604  val_accuracy: 0.497\n",
      "train loss: 100%[**************************************************->]1.346 \n",
      "89.97039204835892\n",
      "[epoch 84] train_loss: 1.574  val_accuracy: 0.507\n",
      "train loss: 100%[**************************************************->]1.435 \n",
      "89.79200978577137\n",
      "[epoch 85] train_loss: 1.585  val_accuracy: 0.498\n",
      "train loss: 100%[**************************************************->]1.939 \n",
      "90.87342115491629\n",
      "[epoch 86] train_loss: 1.572  val_accuracy: 0.506\n",
      "train loss: 100%[**************************************************->]1.506 \n",
      "91.61922622472048\n",
      "[epoch 87] train_loss: 1.553  val_accuracy: 0.513\n",
      "train loss: 100%[**************************************************->]1.487 \n",
      "89.29654451459646\n",
      "[epoch 88] train_loss: 1.543  val_accuracy: 0.529\n",
      "train loss: 100%[**************************************************->]1.418 \n",
      "90.4661765396595\n",
      "[epoch 89] train_loss: 1.536  val_accuracy: 0.515\n",
      "train loss: 100%[**************************************************->]1.804 \n",
      "91.27197755873203\n",
      "[epoch 90] train_loss: 1.538  val_accuracy: 0.518\n",
      "train loss: 100%[**************************************************->]1.370 \n",
      "88.8938469812274\n",
      "[epoch 91] train_loss: 1.523  val_accuracy: 0.523\n",
      "train loss: 100%[**************************************************->]1.928 \n",
      "90.4805830270052\n",
      "[epoch 92] train_loss: 1.511  val_accuracy: 0.525\n",
      "train loss: 100%[**************************************************->]1.738 \n",
      "90.83967135846615\n",
      "[epoch 93] train_loss: 1.514  val_accuracy: 0.520\n",
      "train loss: 100%[**************************************************->]1.669 \n",
      "90.59957972168922\n",
      "[epoch 94] train_loss: 1.533  val_accuracy: 0.515\n",
      "train loss: 100%[**************************************************->]1.211 \n",
      "90.31989279389381\n",
      "[epoch 95] train_loss: 1.509  val_accuracy: 0.515\n",
      "train loss: 100%[**************************************************->]1.374 \n",
      "90.8095983415842\n",
      "[epoch 96] train_loss: 1.500  val_accuracy: 0.524\n",
      "train loss: 100%[**************************************************->]1.650 \n",
      "89.50407242774963\n",
      "[epoch 97] train_loss: 1.455  val_accuracy: 0.522\n",
      "train loss: 100%[**************************************************->]1.653 \n",
      "91.74626612663269\n",
      "[epoch 98] train_loss: 1.482  val_accuracy: 0.535\n",
      "train loss: 100%[**************************************************->]1.473 \n",
      "91.82981147617102\n",
      "[epoch 99] train_loss: 1.484  val_accuracy: 0.534\n",
      "train loss: 100%[**************************************************->]1.079 \n",
      "90.71353430300951\n",
      "[epoch 100] train_loss: 1.434  val_accuracy: 0.545\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = AlexNet(num_classes=40)\r\n",
    "loss_function = nn.CrossEntropyLoss()\r\n",
    "optimizer = optim.Adam(parameters=net.parameters(), learning_rate=0.0001)\r\n",
    "\r\n",
    "epochs = 100\r\n",
    "save_path = 'AlexNet.pth'\r\n",
    "best_acc = 0.0\r\n",
    "train_steps = len(train_loader)\r\n",
    "for epoch in range(epochs):\r\n",
    "    # train\r\n",
    "    net.train()  # 启用dropout\r\n",
    "    running_loss = 0.0\r\n",
    "    t1 = time.perf_counter()\r\n",
    "    for step, data in enumerate(train_loader, start=0):\r\n",
    "        images, labels = data\r\n",
    "        outputs = net(images)\r\n",
    "        loss = loss_function(outputs, labels)\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.clear_grad() # 梯度置零\r\n",
    "\r\n",
    "        # print statistics\r\n",
    "        running_loss += loss.numpy()[0]\r\n",
    "        rate = (step + 1) / len(train_loader)\r\n",
    "        a = \"*\" * int(rate * 50)\r\n",
    "        b = \".\" * int((1 - rate) * 50)\r\n",
    "        loss_num = loss.numpy()[0]\r\n",
    "        print(\"\\rtrain loss: {:^3.0f}%[{}->{}]{:.3f}\".format(int(rate * 100), a, b, loss_num), end=\" \")\r\n",
    "    print()\r\n",
    "    print(time.perf_counter()-t1)\r\n",
    "\r\n",
    "    # validate\r\n",
    "    net.eval()  # 关闭dropout\r\n",
    "    acc = 0.0  # accumulate accurate number / epoch\r\n",
    "    with paddle.no_grad():  # 禁止跟踪\r\n",
    "        for val_data in validate_loader:\r\n",
    "            val_images, val_labels = val_data\r\n",
    "            outputs = net(val_images)\r\n",
    "            predict_y = outputs.numpy().argmax(axis=1)\r\n",
    "            acc += (val_labels.astype(\"int\").numpy()==outputs.numpy().argmax(axis=1).astype(\"int\")).sum()\r\n",
    "\r\n",
    "        val_accurate = acc / val_num\r\n",
    "        print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %\r\n",
    "              (epoch + 1, running_loss / train_steps, val_accurate))\r\n",
    "\r\n",
    "        if val_accurate > best_acc:\r\n",
    "            best_acc = val_accurate\r\n",
    "            paddle.save(net.state_dict(), save_path)\r\n",
    "\r\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"data/data35095/test.zip\") as zf:\r\n",
    "   zf.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    }
   ],
   "source": [
    "paddle.jit.save(\r\n",
    "    layer=net,\r\n",
    "    path=\"model\",\r\n",
    "    input_spec=[InputSpec(shape=[1, 3, 224, 224], dtype='float32')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process: 17% [********->.........................................]  "
     ]
    }
   ],
   "source": [
    "data_transform = transforms.Compose(\r\n",
    "    [transforms.Resize((224, 224)),\r\n",
    "     transforms.ToTensor(),\r\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\r\n",
    "\r\n",
    "# load image\r\n",
    "with open(\"test.txt\",\"w\") as f:\r\n",
    "    for i in range(1,401):\r\n",
    "        img_path = \"test/test\"+str(i)+\".jpg\"\r\n",
    "        assert os.path.exists(img_path), \"file: '{}' dose not exist.\".format(img_path)\r\n",
    "        img = Image.open(img_path)\r\n",
    "\r\n",
    "        paddle.disable_static()\r\n",
    "        # plt.imshow(img)\r\n",
    "        # [N, C, H, W]\r\n",
    "        img = data_transform(img)\r\n",
    "        # expand batch dimension\r\n",
    "        img = paddle.unsqueeze(img, axis=0)\r\n",
    "\r\n",
    "        # read class_indict\r\n",
    "        json_path = 'class_indices.json'\r\n",
    "        assert os.path.exists(json_path), \"file: '{}' dose not exist.\".format(json_path)\r\n",
    "\r\n",
    "        json_file = open(json_path, \"r\")\r\n",
    "        class_indict = json.load(json_file)\r\n",
    "\r\n",
    "        # create model\r\n",
    "        model = AlexNet(num_classes=40)\r\n",
    "\r\n",
    "        # load model weights\r\n",
    "        # weights_path = \"AlexNet.pth\"\r\n",
    "        # assert os.path.exists(weights_path), \"file: '{}' dose not exist.\".format(weights_path)\r\n",
    "        model = paddle.jit.load(\"model\")\r\n",
    "        # model.load_state_dict(torch.load(weights_path))\r\n",
    "\r\n",
    "        model.eval()\r\n",
    "        with paddle.no_grad():\r\n",
    "            # predict class\r\n",
    "            output = paddle.squeeze(model(img))\r\n",
    "            predict = nn.Softmax(axis=0)(output)\r\n",
    "            predict_cla = paddle.argmax(predict).numpy()[0]\r\n",
    "\r\n",
    "        # print_res = \"class: {}   prob: {:.3}\".format(class_indict[str(predict_cla)],\r\n",
    "                                                    #predict.numpy()[predict_cla])\r\n",
    "        # print(print_res)\r\n",
    "    \r\n",
    "        f.write(class_indict[str(predict_cla)]+\"\\n\")\r\n",
    "\r\n",
    "        rateTest = i / 400\r\n",
    "        aTest = \"*\" * int(rateTest * 50)\r\n",
    "        bTest = \".\" * int((1 - rateTest) * 50)\r\n",
    "        print(\"\\rprocess: {:d}% [{}->{}]\".format(int(rateTest * 100), aTest, bTest), end=\" \")\r\n",
    "\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
